{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerai Stock Market Prediction \n",
    "\n",
    "## About\n",
    "Numerai is a hedge fund that invests its capital by hosting weekly competitions to predict market movements.  Competitors submit predictions and Numerai ranks the predictions based on their novelty compared to other predictions and their accuracy on historical data.  Contained in the competition test data are also future unknown events.  Numerai ensembles the top competitor predictions based on the historical known data to determine where to allocate capital for the future unknown events. \n",
    "\n",
    "\n",
    "## Problem Discussion\n",
    "\n",
    "Numerai provides a weekly dataset with features that have been anonymized to predict a binary 'yes-no target'.  Numerai rewards predictions based both on the models accuracy (logloss metric) and on the model novelty (based on the model correlation with other model predictions).  The logic behind rewarding novel models is that non-correlated models provide new information to the Numerai's meta model.   When the novel models are combined with other top performing models the end result will be a still more accurate model.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Numerai allocates their capital to models that place in the top 100 out of the typically more than 600 competitors.  The goal of this project is to produce a model that scores in the top 100 meaning that the model actually has real money deployed.    To place in the top 100 on the leaderboard over the past few weeks translates into a logloss score of ~.685xx.  To score well on the novelty metric we would need to know what the predictions of other submissions and this data is not provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tsne import bh_sne\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the raw data provided weekly by [Numerai](https://numer.ai/about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136573, 51)\n",
      "(90403, 51)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_data = pd.read_csv(\"/Users/jeremycastle/Desktop/numerai/week37/numerai_datasets/numerai_training_data.csv\")\n",
    "prediction_data = pd.read_csv(\"/Users/jeremycastle/Desktop/numerai/week37/numerai_datasets/numerai_tournament_data.csv\")\n",
    "\n",
    "print (training_data.shape)\n",
    "print (prediction_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, for reasons that are still trying to be ascertained by the Numerai competitor community, the prediction data has in the past contained a copy of the training data.  In this step, duplicate training data contained in the prediction data will be removed.\n",
    "\n",
    "Also, the prediction target variable will be seperated the predictive features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136573, 50)\n",
      "(90403, 50)\n",
      "136573\n",
      "90403\n"
     ]
    }
   ],
   "source": [
    "# Seperate training data from test data\n",
    "merged = prediction_data.merge(training_data, how='left', indicator=True)\n",
    "prediction_data = prediction_data[merged['_merge'] == 'left_only']\n",
    "\n",
    "# Seperate the target data and test_id to create clean target and features\n",
    "target = training_data['target']\n",
    "t_id = prediction_data['t_id']\n",
    "t_id.reset_index(drop=True, inplace=True)\n",
    "\n",
    "del training_data['target']\n",
    "del prediction_data['t_id']\n",
    "\n",
    "print (training_data.shape)\n",
    "print (prediction_data.shape)\n",
    "print (len(target))\n",
    "print (len(t_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model\n",
    "To provide an initial baseline we begin by thinking of a naive model that predicts that each outcome is equally likely or basically, a 50-50 situation.  A model that predicts a 50 percent chance of the event occurring would produce a logloss of ~.69315.  The first model that we will explore is the simple logistic regression model without altering the provided data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create testing data\n",
    "Competitors are provided training data where the correct outcome is known, so initially we will break this into two sets.  The first set will be used to train a model.  The second set of data will not be used in training at all and will be only used to test how accurate the model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset in two equal parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, target, stratify= target, test_size=0.15, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression Model\n",
    "Run a logistic regression using sklearn and calculate the models score on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for Logistic Regression: 0.691844446192\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LogisticRegressionCV(random_state=23)\n",
    "\n",
    "model = reg.fit(X_train,y_train)\n",
    "Y_pred = model.predict_proba(X_test)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "score_log_reg = log_loss(y_test, Y_pred)\n",
    "print (\"Logloss for Logistic Regression:\", score_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Results\n",
    "An extremely simple linear model without feature engineering produces a logloss of 0.69184 or a 0.3% improvement over the naive model.  Not a great result, but at least it is better than guessing.  The above reported logloss will be slightly different if the notebook is run on a different weekly dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components\n",
    "There has been discussion in the Slack discussion group that all variation in the 50 features in the training set can be explained in 21 principal components and all the variation in the 50 features in the prediction data is explained by 43 principal components.  This indicates that there is significant difference in the training and predition datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data variance... [ 0.29713625  0.4575837   0.58448885  0.68599758  0.75917535  0.81249901\n",
      "  0.84979085  0.88336927  0.90640096  0.92576736  0.94003582  0.95176521\n",
      "  0.96283854  0.97151542  0.9791538   0.98545869  0.98967669  0.9932504\n",
      "  0.99610821  0.99858714  1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          1.        ]\n",
      "Prediction data variance... [ 0.2568876   0.3942072   0.50780297  0.598927    0.66580281  0.71836574\n",
      "  0.75932074  0.78990753  0.8188255   0.84364088  0.86430582  0.88157187\n",
      "  0.89611634  0.90954548  0.9217099   0.93247391  0.94142078  0.94991658\n",
      "  0.95703345  0.96393763  0.96975982  0.97395798  0.97792859  0.9814558\n",
      "  0.98417758  0.98661911  0.98880686  0.99060356  0.9923319   0.99395854\n",
      "  0.99523825  0.99624064  0.99714987  0.99784102  0.99843259  0.99893243\n",
      "  0.9992987   0.99957166  0.99974882  0.99985273  0.99991969  0.99996509\n",
      "  1.          1.          1.          1.          1.          1.          1.\n",
      "  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Apply PCA by fitting the good data with the same number of dimensions as features\n",
    "pca = PCA(n_components=50, random_state=341)\n",
    "pca1 = PCA(n_components=50, random_state=341)\n",
    "pca_train_model = pca.fit(training_data)\n",
    "pca_prediction_model = pca1.fit(prediction_data)\n",
    "\n",
    "# Makes no sense why when I use one pca line the explained variance is the same for both datasets, but when pca \n",
    "# and pca1 are used the expected principal components are returned.\n",
    "\n",
    "\n",
    "# Cumulative explained variance\n",
    "explained_training_variance = np.cumsum(pca_train_model.explained_variance_ratio_)\n",
    "explained_prediction_variance = np.cumsum(pca_prediction_model.explained_variance_ratio_)\n",
    "\n",
    "print (\"Training data variance...\", explained_training_variance)\n",
    "print (\"Prediction data variance...\", explained_prediction_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Dos:\n",
    "1. Investigate techniques to address large differences in training and testing set features\n",
    "2. Figure out note in above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Old Code Below***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Non-Linearity and Interaction Between Features\n",
    "The Logistic Regression is extremely simple (though powerful) linear model that does not take into account interaction between terms.  Next we will engineer features to include interactions between all the terms and add non-linearity by squaring the existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96320, 252)\n"
     ]
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "polyfeatures = pd.DataFrame(poly.fit_transform(train))\n",
    "# drop first column that only contains 1's\n",
    "polyfeatures = polyfeatures.drop(polyfeatures.columns[0], axis=1)\n",
    "print polyfeatures.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have now increased the number of features from 21 to 252.  We will rerun the Logistic Regression model on the new set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logloss for Logistic Regression: 0.691143753005\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into a 85% traning and 15% testing set\n",
    "X_trainp, X_testp, y_trainp, y_testp = train_test_split(polyfeatures, target, stratify= target, test_size=0.15, random_state=20)\n",
    "\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LogisticRegressionCV(random_state=1, n_jobs=-1)\n",
    "\n",
    "model = reg.fit(X_trainp,y_trainp)\n",
    "Y_predp = model.predict_proba(X_testp)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "score_log_reg = log_loss(y_testp, Y_predp)\n",
    "print \"Logloss for Logistic Regression:\", score_log_reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Linearity and Interaction Between Features Results\n",
    "Adding the squared and interaction features produces a logloss of 0.69114.  No real improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and t-SNE\n",
    "Since the non-linearity and interaction features did not provide an increase we will move onto two feature-engineering techniques.  PCA and t-SNE are methods that mathematically produce simpler representations of the feature space.  These techniques reduce the amount of noise in the data and can provide a cleaner signal for a learning algorithm to identify.  Hopefully the clearer signal produces better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate TSNE features\n",
    "def features_tsne(X):\n",
    "    features_tsne = pd.DataFrame(bh_sne(X))\n",
    "    features_tsne.reset_index(drop=True, inplace=True)\n",
    "    return features_tsne\n",
    "\n",
    "# calculate PCA features\n",
    "def features_pca(X):\n",
    "    pca = decomposition.PCA(n_components=2, random_state=100)\n",
    "    features_pca = pd.DataFrame(pca.fit_transform(X))\n",
    "    features_pca.reset_index(drop=True, inplace=True)\n",
    "    return features_pca\n",
    "\n",
    "# create polynomial and interaction features\n",
    "def features_poly(X):\n",
    "    poly = PolynomialFeatures(2)\n",
    "    polyfeatures = pd.DataFrame(poly.fit_transform(X))\n",
    "    # drop first column that only contains 1's\n",
    "    polyfeatures = pd.DataFrame(polyfeatures.drop(polyfeatures.columns[0], axis=1))\n",
    "    print polyfeatures.shape\n",
    "    return polyfeatures\n",
    "\n",
    "# create scale features\n",
    "def features_scale(X):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    X_scaled = pd.DataFrame(min_max_scaler.fit_transform(X))\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Test and Train Features to Create PCA and tSNE Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135259, 21)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.concat([train, test], axis=0)\n",
    "all_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating tsne features\n",
      "()\n",
      "calculating pca features\n",
      "()\n",
      "(135259, 350)\n",
      "Training model . . .\n",
      "()\n",
      "Creating training predictions . . .\n",
      "()\n",
      "Logloss for LogisticRegressionCV:  0.689401056926\n"
     ]
    }
   ],
   "source": [
    "print \"calculating tsne features\"\n",
    "print ()\n",
    "f_tsne = features_tsne(all_features)\n",
    "\n",
    "print \"calculating pca features\"\n",
    "print ()\n",
    "f_pca = features_pca(all_features)\n",
    "\n",
    "# merge new tsne and pca features with the original features\n",
    "features_tsne_pca_all = pd.concat([all_features, f_tsne, f_pca], axis=1) \n",
    "\n",
    "# create squared and interaction features\n",
    "f_poly = features_poly(features_tsne_pca_all)\n",
    "\n",
    "# scale features \n",
    "f_scaled = features_scale(f_poly)\n",
    "\n",
    "# seperate the testing and training features\n",
    "train_big_features = f_scaled.ix[0:(len(train)-1),]\n",
    "test_big_features = f_scaled.ix[(len(train)):(len(f_scaled)-1),]\n",
    "\n",
    "# split the training dataset into a 85% training and 15% testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_big_features, target, stratify= target, test_size=0.15, random_state=500)\n",
    "\n",
    "# set algorithym to logistic regression\n",
    "reg = linear_model.LogisticRegressionCV(random_state=200, n_jobs=-1)\n",
    "\n",
    "print \"Training model . . .\"\n",
    "print ()\n",
    "model = reg.fit(X_train,y_train)\n",
    "print \"Creating training predictions . . .\"\n",
    "print ()\n",
    "Y_predp = model.predict_proba(X_test)\n",
    "score_lr = log_loss(y_test,Y_predp)\n",
    "print \"Logloss for LogisticRegressionCV: \", score_lr    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results with PCA and tSNE features\n",
    "Creating tSNE features takes about 25 minutes.  The new model with produces a logloss of .0.6894 just shy of the project goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Step - Multiple Model Runs in a Loop\n",
    "The final step will be to run the same model three times over different subsets of the training data and with three different runs of the t-SNE.  The separate t-SNE runs give a different representation of the original features each time so this will add slightly different information for the algorithm to pick up on each time.  Averaging these three runs should increase the predictive power of out model and provide more robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating tsne features\n",
      "calculating pca features\n",
      "(135259, 350)\n",
      "Training model . . .\n",
      "Creating training predictions . . .\n",
      "Logloss for LogisticRegressionCV:  0.689600470573\n",
      "Creating testing predictions . . .\n",
      "()\n",
      "calculating tsne features\n",
      "calculating pca features\n",
      "(135259, 350)\n",
      "Training model . . .\n",
      "Creating training predictions . . .\n",
      "Logloss for LogisticRegressionCV:  0.688055670367\n",
      "Creating testing predictions . . .\n",
      "()\n",
      "calculating tsne features\n",
      "calculating pca features\n",
      "(135259, 350)\n",
      "Training model . . .\n",
      "Creating training predictions . . .\n",
      "Logloss for LogisticRegressionCV:  0.68716640806\n",
      "Creating testing predictions . . .\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# loop through model\n",
    "predictions = pd.DataFrame()\n",
    "for n in [1,2,3]:\n",
    "\n",
    "    print \"calculating tsne features\"\n",
    "    f_tsne = features_tsne(all_features)\n",
    "\n",
    "    print \"calculating pca features\"\n",
    "    f_pca = features_pca(all_features)\n",
    "\n",
    "    # merge new tsne and pca features with the original features\n",
    "    features_tsne_pca_all = pd.concat([all_features, f_tsne, f_pca], axis=1) \n",
    "\n",
    "    # create squared and interaction features\n",
    "    f_poly = features_poly(features_tsne_pca_all)\n",
    "\n",
    "    # scale features \n",
    "    f_scaled = features_scale(f_poly)\n",
    "\n",
    "    # seperate the testing and training features\n",
    "    train_big_features = f_scaled.ix[0:(len(train)-1),]\n",
    "    test_big_features = f_scaled.ix[(len(train)):(len(f_scaled)-1),]\n",
    "\n",
    "    # split the training dataset into a 85% training and 15% testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_big_features, target, stratify= target, test_size=0.15, random_state=500+n)\n",
    "\n",
    "    # set algorithym to logistic regression\n",
    "    reg = linear_model.LogisticRegressionCV(random_state=200+n, n_jobs=-1)\n",
    "\n",
    "    print \"Training model . . .\"\n",
    "    model = reg.fit(X_train,y_train)\n",
    "    print \"Creating training predictions . . .\"\n",
    "    Y_predp = model.predict_proba(X_test)\n",
    "    score_lr = log_loss(y_test,Y_predp)\n",
    "    print \"Logloss for LogisticRegressionCV: \", score_lr\n",
    "\n",
    "    print \"Creating testing predictions . . .\"\n",
    "    print ()    \n",
    "    predictions['pred'+str(n)] = model.predict_proba(test_big_features)[:,1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Results\n",
    "The above three runs produce logloss scores of 0.68960, 0.68805,and 0.68716 and take approximately an hour to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating a CSV file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# take the arithmetic mean of three predictions\n",
    "predictions['probability'] = predictions.mean(axis=1)\n",
    "\n",
    "# combine the row ids with the predictions\n",
    "submit = pd.concat([t_id,predictions['probability']], axis=1)\n",
    "\n",
    "# add in the training rows to the test set as Numerai only excepts complete testing data with the included training data\n",
    "merged1 = merged.merge(submit, how = 'outer', on='t_id')\n",
    "merged1['probability'].fillna(merged1['target'], inplace=True)\n",
    "\n",
    "# take only the columns of interest from the merged data\n",
    "finalsubmit = merged1[['t_id', 'probability']]\n",
    "\n",
    "# create CSV file\n",
    "finalsubmit.to_csv('.../submission/logregcv_loopn1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Submission Result\n",
    "The final submission scored 0.68965 on Numerai’s public leaderboard and has floated between 38th and 120th.  The submission consistently ranks in the 50th-70th place range out of the current 400 competitor submissions.  The goal of producing a model that is in the Top 100 and commands capital has been achieved at least on the public leaderboard.  Will report back next Wednesday (December 7th) to report on the final private leaderboard results.\n",
    "\n",
    "## Final Results\n",
    "Through the final three days of the submission continued to slide down the leader board and finished 166/647.  Need to revisit the below steps in the future for improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Future Steps\n",
    "1. The majority of the code runtime comes from calculating the t-SNE features.  There exists a parallel t-SNE module that I have attempted to install, but have not been able run.  Hopefully this parallel module can reduce the runtime considerably.\n",
    "2. Experiment with different settings in t-SNE, specifically the perplexity features.  \n",
    "3. There are a number of top 10 competitors producing predictions 20-30 minutes after new data sets are released.  If top 10 predictions are attainable in 30 minutes this should be the time goal.\n",
    "3. Automate the downloading of the data from Numerai and the uploading of predictions.\n",
    "4. Explore stacking and ensembling different types of models.\n",
    "5. Create a validation set to properly score the average of the three models without relying on the Numerai leaderboard to assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
